import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os

# Load model-ready data
X = np.load("model_data/X.npy")
y = np.load("model_data/y.npy")

print(f"âœ… Loaded X: {X.shape}, y: {y.shape}")

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train XGBoost regressor
model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    tree_method="hist"  # Use "gpu_hist" if you have a GPU
)

print("ðŸš€ Training XGBoost model...")
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nðŸ“Š Evaluation Results:")
print(f"MAE:  {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"RÂ²:   {r2:.4f}")

# Optional: save the model
os.makedirs("models", exist_ok=True)
model.save_model("models/xgboost_baseline.json")
print("âœ… Model saved to models/xgboost_baseline.json")
